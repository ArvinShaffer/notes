[TOC]

# Introduction

ä»€ä¹ˆæ˜¯ä¸‰ç»´ç‚¹äº‘ï¼Ÿ

![0003.3DPointCloud](1301pclpics/0003.3DPointCloud.png)

**ç‚¹äº‘çš„åº”ç”¨**

- Robotics, Autonomous driving
  - Localization â€“ SLAM, loop closure, registration æœ¬åœ°åŒ– â€“ SLAMã€é—­ç¯ã€æ³¨å†Œ
  - Perception â€“ object detection, classification æ„ŸçŸ¥â€”â€”ç‰©ä½“æ£€æµ‹ã€åˆ†ç±»
  - Reconstruction â€“ SfM, registration  é‡å»º - SfMï¼Œæ³¨å†Œ

- Consumer Electronics  æ¶ˆè´¹ç±»ç”µå­äº§å“
  - Face detection / reconstruction â€“ FaceID  äººè„¸æ£€æµ‹/é‡å»º - FaceID
  - Hand pose â€“ Hololens  æ‰‹éƒ¨å§¿åŠ¿ - Hololens
  - Human pose â€“ Kinect  äººä½“å§¿åŠ¿ - Kinect

**ä¼˜åŠ¿**

3D æœ‰å“ªäº›é€‰æ‹©ï¼Ÿ

![0004.OptionsFor3D](1301pclpics/0004.OptionsFor3D.png)

ç‚¹äº‘çš„å¼ºåº¦

- 3D ä¿¡æ¯
- æ•°å­¦ä¸Šç®€å•æ˜äº†

**å›°éš¾**
ç‚¹äº‘å¤„ç†éš¾ç‚¹

- ç¨€ç–æ€§  Sparsity
- ä¸è§„åˆ™ - é‚»å±…æœç´¢å›°éš¾ Irregular â€“ difficulty in neighbor searching
- ç¼ºä¹çº¹ç†ä¿¡æ¯ Lack of texture information
- æ— åºâ€”â€”æ·±åº¦å­¦ä¹ çš„éš¾ç‚¹ Un-ordered â€“ difficulty in deep learning
- æ—‹è½¬ç­‰æ–¹å·®/ä¸å˜æ€§ Rotation equivariance / invariance

**Classical Methods**

- Pros ä¼˜ç‚¹
  - Explainable â€“ It follows physics and we know why it works/doesnâ€™t work å¯è§£é‡Šâ€”â€”å®ƒéµå¾ªç‰©ç†å­¦ï¼Œæˆ‘ä»¬çŸ¥é“å®ƒä¸ºä»€ä¹ˆæœ‰æ•ˆ/æ— æ•ˆ
  - Controllable â€“ We know how to debug å¯æ§â€”â€”æˆ‘ä»¬çŸ¥é“å¦‚ä½•è°ƒè¯•
- Cons ç¼ºç‚¹
  - Hard to model semantics  éš¾ä»¥å»ºæ¨¡è¯­ä¹‰
  - User-unfriendly  ç”¨æˆ·ä¸å‹å¥½

**Deep Learning Methods**

- Pros
  - Simple!
  - High performance  é«˜æ€§èƒ½
  - Data driven æ•°æ®é©±åŠ¨
- Cons
  - Un-explainable â€“ No one knows why / how.  æ— æ³•è§£é‡Šâ€”â€”æ²¡æœ‰äººçŸ¥é“ä¸ºä»€ä¹ˆ/å¦‚ä½•ã€‚
  - Un-controllable â€“ Black box  ä¸å¯æ§â€”â€”é»‘åŒ£å­
  - Requires special hardware â€“ GPU / FPGA, etc.  éœ€è¦ç‰¹æ®Šçš„ç¡¬ä»¶â€”â€”GPU / FPGA ç­‰ã€‚
  - Simple â€“ The barrier is lower and lower means it will be more and more difficult to find a job. ç®€å•â€”â€”é—¨æ§›è¶Šæ¥è¶Šä½æ„å‘³ç€æ‰¾å·¥ä½œä¼šè¶Šæ¥è¶Šéš¾ã€‚

**ç»å…¸æ–¹æ³• VS æ·±åº¦å­¦ä¹ **

- å¯¹è±¡åˆ†ç±»
  - ç»å…¸æ–¹æ³•
    - Keypoint detection å…³é”®ç‚¹æ£€æµ‹ 
    - Keypoint description å…³é”®ç‚¹æè¿° 
    - Support Vector Machine æ”¯æŒå‘é‡æœº
  - æ·±åº¦å­¦ä¹ 
    - Data collection æ•°æ®æ”¶é›†
    - Data labeling æ•°æ®æ ‡è®°
    - Train a network è®­ç»ƒç½‘ç»œ

- å¯¹è±¡é…å‡†
  - ç»å…¸æ–¹æ³•
    - Nearest Neighbor Search æœ€è¿‘é‚»æœç´¢
    - Iterative Closest Point è¿­ä»£æœ€è¿‘ç‚¹
  - æ·±åº¦å­¦ä¹ 
    - Data collection æ•°æ®æ”¶é›†
    - Data labeling æ•°æ®æ ‡è®°
    - Train a network è®­ç»ƒç½‘ç»œ

- å¯¹è±¡æ£€æµ‹
  - ç»å…¸æ–¹æ³•
    -  Background removal èƒŒæ™¯å»é™¤
    -  Clustering èšç±»
    - Classification åˆ†ç±»
  - æ·±åº¦å­¦ä¹ 
    - Data collection æ•°æ®æ”¶é›†
    - Data labeling æ•°æ®æ ‡è®°
    - Train a network è®­ç»ƒç½‘ç»œ

**Common Tools for Practice**

- C++
  - Point Cloud Library (PCL)
  - Python Binding â€“ pybind11
  - Optimization Solver â€“ g2o, Ceres
  - Eigen
- Python
  - numpy
  - scipy
  - Open3D
  - Pytorch
  - Tensorflow

# PCA

**Principle Component Analysis ä¸»æˆåˆ†åˆ†æ**

- PCA is to find the dominant directions of thepoint cloud PCA æ˜¯å¯»æ‰¾ç‚¹äº‘çš„ä¸»å¯¼æ–¹å‘
- Applications:
  - Dimensionality reduction é™ç»´
  - Surface normal estimation æ³•å‘é‡ä¼°è®¡
  - Canonical orientation  è§„èŒƒå–å‘
  - Keypoint detection  å…³é”®ç‚¹æ£€æµ‹
  - Feature description åŠŸèƒ½æè¿°

![0005.PCA](1301pclpics/0005.PCA.png)

- çŸ¢é‡ç‚¹ç§¯ Vector Dot Product
- çŸ©é˜µå‘é‡ä¹˜æ³• Matrix-Vector Multiplication
- å¥‡å¼‚å€¼åˆ†è§£ (SVD) Singular Value Decomposition (SVD)

![0006.PhysicalIntuitions](1301pclpics/0006.PhysicalIntuitions.png)

**Spectral Theorem è°±å®šç†**

Let $A \in R^{n,n}$ be symmetric, and $\lambda_i \in R, i = 1,2,\cdots,n$ be the eigenvalues of $A$ . There exists a set of orthonormal vectors  $u_i \in R_n, i = 1,2,\cdots,n$,  such that  $Au_i = \lambda_iu_i$,  Equivalently, there exists an orthogonal matrix  $U = [u_1, \cdots, u_n](i.e., UU^T = U^TU = I_n)$,  such that ,

è®¾ $A \in R^{n,n}$æ˜¯å¯¹ç§°çš„ï¼Œå¹¶ä¸” $\lambda_i \in R, i = 1,2,\cdots,n$ æ˜¯ $A$ çš„ç‰¹å¾å€¼ã€‚å­˜åœ¨ä¸€ç»„æ­£äº¤å‘é‡ $u_i \in R_n, i = 1,2,\cdots,n$ï¼Œä½¿å¾— $Au_i = \lambda_iu_i$ã€‚ç­‰ä»·åœ°ï¼Œå­˜åœ¨ä¸€ä¸ªæ­£äº¤çŸ©é˜µ $U = [u_1, \cdots, u_n](i.e., UU^T = U^TU = I_n)$ï¼Œä½¿å¾—ï¼Œ
$$
A = U \Lambda U^T = \sum_{i=1}^{n} \lambda_i u_i u_i^T, \Lambda = diag(\lambda_1,\cdots, \lambda_n)
$$

**Rayleigh Quotients ç‘åˆ©å•†**

Physical meaning of SVD! 

å¥‡å¼‚å€¼åˆ†è§£(Singular Value Decompositionï¼Œä»¥ä¸‹ç®€ç§°SVD)

ç»™å®šä¸€ä¸ªå¯¹ç§°çŸ©é˜µ Given a symmertric matrix $A \in S^n$, 
$$
\lambda_{min}(A) \leq \frac{x^TAx}{x^Tx} \leq \lambda_{max}(A), \forall x \neq 0 \\
\lambda_{max}(A) = \underset{x:\|x\|_2=1}{max} x^TAx\\
\lambda_{min}(A) = \underset{x:\|x\|_2 = 1}{min} x^TAx
$$
The maximum and minimum are attained for $x = u_1$ and for $x = u_n$, respectively,  where $u_1$ and $u_n$ are the largest and smallest eigenvector of $A$, respectively.

åˆ†åˆ«ä¸º $x = u_1$ å’Œ $x = u_n$ è·å¾—æœ€å¤§å€¼å’Œæœ€å°å€¼ï¼Œå…¶ä¸­ $u_1$ å’Œ $u_n$ åˆ†åˆ«æ˜¯ $A$ çš„æœ€å¤§å’Œæœ€å°ç‰¹å¾å‘é‡ã€‚

**ç‘åˆ©å•†â€”â€”è¯æ˜ï¼š**

- Apply the spectral theorem, $U$ is orthogonal, $\Lambda$ is diagonal
- åº”ç”¨è°±å®šç†ï¼Œ$U$æ˜¯æ­£äº¤çš„ï¼Œ$\Lambda$ æ˜¯å¯¹è§’çº¿

$$
X^TAx = x^T U \Lambda U^T x = \bar{x}^T \Lambda \bar{x} = \sum_{i=1}^{n} \lambda_i \bar{x}^2_i
$$

- Obviously

$$
\lambda_{min} \sum_{i=1}^n \bar{x}_i^2 \leq \sum_{i=1}^2 \lambda_i\bar{x}^2_i \leq \lambda_{max} \sum_{i=1}^n \bar{x}^2_i
$$

- Also, orthogonal matrix $U$ doesnâ€™t change the norm of any vector
- æ­¤å¤–ï¼Œæ­£äº¤çŸ©é˜µ $U$ ä¸ä¼šæ”¹å˜ä»»ä½•å‘é‡çš„èŒƒæ•°

$$
\sum_{i=1}^n x^2_i = x^Tx = x^TUU^Tx = (U^Tx)^T(U^Tx) = \bar{x}^T\bar{x} = \sum_{i=1}^n \bar{x}^2_i
$$

- Combining the above 3 equations,

$$
\lambda_{min} x^T x \leq x^T A x \leq \lambda_{max}x^Tx
$$

Input: $x_i \in \mathbb{R}^n, i = 1, 2, \cdots, m$

Output: principle vectors ä¸»å‘é‡$z_1,z_2,\cdots,z_k \in \mathbb{R}^n, k \leq n$ 

ä¸»æˆåˆ†åˆ†æ

- Q: What is the most significant principle component?
  A: A direction such that the variance of the projected data points on that direction is maximal.

- é—®ï¼šæœ€é‡è¦çš„ä¸»æˆåˆ†æ˜¯ä»€ä¹ˆï¼Ÿ
  ç­”ï¼šä¸€ä¸ªæ–¹å‘ï¼Œä½¿å¾—è¯¥æ–¹å‘ä¸Šçš„æŠ•å½±æ•°æ®ç‚¹çš„æ–¹å·®æœ€å¤§ã€‚

- Q: How to get the second significant one?
  A: Deflation. Remove the most significant component from the data points, i.e., data point minus the projection. Find the most significant component for the deflated data.
- é—®ï¼šå¦‚ä½•è·å¾—ç¬¬äºŒä¸ªé‡è¦çš„ï¼Ÿ
  ç­”ï¼šé€šè´§ç´§ç¼©ã€‚ ä»æ•°æ®ç‚¹ä¸­åˆ é™¤æœ€é‡è¦çš„ç»„æˆéƒ¨åˆ†ï¼Œå³æ•°æ®ç‚¹å‡å»æŠ•å½±ã€‚ ä¸ºå‹ç¼©çš„æ•°æ®æ‰¾åˆ°æœ€é‡è¦çš„ç»„æˆéƒ¨åˆ†ã€‚

- Q: How to get the $3^{rd}$ one?
  A: Repeat the above steps.
- é—®ï¼šå¦‚ä½•è·å¾—ç¬¬ä¸‰ä¸ªï¼Ÿ
  ç­”ï¼šé‡å¤ä»¥ä¸Šæ­¥éª¤ã€‚

**Principle Component Analysis - Proof**

- Normalize the data to be zero mean
- å°†æ•°æ®å½’ä¸€åŒ–ä¸ºé›¶å‡å€¼

$$
\tilde{X} = [\tilde{x}_1, \cdots, \tilde{x}_m], \tilde{x}_i = x_i - \bar{x}, i = 1, \cdots , m \qquad \bar{x} = \frac{1}{m} \sum^{m}_{i=1}x_i
$$

- PCA is to get largest variance when projected to a direction $z \in \mathbb{R}^n,\ \|z\|_2 = 1$
- PCA æ˜¯åœ¨æŠ•å½±åˆ°ä¸€ä¸ªæ–¹å‘æ—¶è·å¾—æœ€å¤§çš„æ–¹å·®  $z \in \mathbb{R}^n,\ \|z\|_2 = 1$

$$
\alpha_i = \tilde{x}_i^T z, \ i = 1, \cdots, m
$$

- The mean variance of the projections is
- æŠ•å½±çš„å¹³å‡æ–¹å·®ä¸º

$$
\frac{1}{m} \sum_{i=1}^m \alpha_i^2 = \frac{1}{m} \sum_{i=1}^m z^T \tilde{x}_i \tilde{x}_i^T z = \frac{1}{m} z^T \tilde{X} \tilde{X}^Tz
$$

- So, maximize it,
- æ‰€ä»¥ï¼Œæœ€å¤§åŒ–å®ƒï¼Œ

$$
\underset{z \in R^n}{max} z^T (\tilde{X}\tilde{X}^T) z, s.t. :\|z\|_2 = 1
$$

- Now, maximize this
- ç°åœ¨ï¼Œæœ€å¤§åŒ–è¿™ä¸ª

$$
\underset{z \in R^n}{max}\ z^T (\tilde{X}\tilde{X}^T)z, \ s.t.:\|z\|_2 = 1
$$

- Recall the Rayleigh Quotients
- å›å¿†ç‘åˆ©å•†æ•°

$$
\lambda_{min}(A) \leq \frac{x^TAx}{x^T x} \leq \lambda_{max}(A), \forall x \neq 0
$$

- Recall our Spectral Theorem
- å›æƒ³ä¸€ä¸‹æˆ‘ä»¬çš„è°±å®šç†

$$
A = U \Lambda U^T = \sum_{i=1}^n \lambda_i u_i u_i^T, \Lambda = diag(\lambda_1, \cdots , \lambda_n)
$$

- Apply to PCA
- ç”³è¯·PCA

$$
H = \tilde{X}\tilde{X}^T = U_r \Sigma^2 U_r^T
$$

- First principle vector $ğ‘§_1 = ğ‘¢_1$ , $ğ‘¢_1$ is the first column of $ğ‘ˆ_ğ‘Ÿ$
- ç¬¬ä¸€åŸç†å‘é‡ $ğ‘§_1 = ğ‘¢_1$ , $ğ‘¢_1$ æ˜¯ $ğ‘ˆ_ğ‘Ÿ$ çš„ç¬¬ä¸€åˆ—

- Let's take a look at $H = \tilde{X}\tilde{X}^T = U_r \Sigma^2U_r^T$.
- Perform SVD on $\tilde{X}$:  $\tilde{X} = U_r\Sigma V_r^T = \sum_{i=1}\sigma_iu_iv_i^T$.
- è°±å®šç†å’ŒSVDå¯†åˆ‡ç›¸å…³

- Find $ğ‘§_2$ by deflation
- é€šè¿‡æ”¶ç¼©æ‰¾åˆ° $z_2$

$$
\tilde{x}_i^{(1)} = \tilde{x}_i - u_1(u_1^T\tilde{x}_i), i = 1, \cdots, m \\
\tilde{X}^{(1)} = [\tilde{x}_1^{(1)}, \cdots ,\tilde{x}_m^{(1)}] = (I_n - u_1u_1^T)\tilde{X}
$$

- Combine the above equations:

$$
\tilde{X}^{(1)} = \sum_{i=1}^r \sigma_i u_i v_i^T - (u_1u_1^T) \sum_{i=1}^r \sigma_i u_i v_i^T \\
= \sum_{i=1}^r \sigma_i u_i v_i^T  - \sum_{i=1}^r \sigma_i u_1 u_1^T u_i v_i^T \\
= \sum_{i=1}^r \sigma_i u_i v_i^T - \sigma_1 u_1 v_1^T // U \ is \ orthogonal \\
= \sum_{i=2}^r \sigma_i u_i v_i^T
$$

- We have removed the first components, finding $ğ‘§_2$ is by
- æˆ‘ä»¬å·²ç»ç§»é™¤äº†ç¬¬ä¸€ä¸ªéƒ¨åˆ†ï¼Œå‘ç° $ğ‘§_2$ æ˜¯ç”±

$$
\underset{z \in R^n}{max} \ z^T (\tilde{X}^{(1)} \tilde{X}^{(1)T})z, \ s.t.:\|z\|_2 = 1 \\
 \tilde{X}^{(1)} = \sum_{i=2}^r \sigma_i u_i v_i^T
$$

- The result is simply $ğ‘§_2 = ğ‘¢_2$ , $ğ‘¢_2$ is the $2^{nd}$ column of $ğ‘ˆ_ğ‘Ÿ$
- ç»“æœå¾ˆç®€å• $ğ‘§_2 = ğ‘¢_2$ , $ğ‘¢_2$ æ˜¯ $ğ‘ˆ_ğ‘Ÿ$ çš„ $2^{nd}$ åˆ—
- $ğ‘§_3, \cdots , ğ‘§_ğ‘š$ can be found by similar deflation.

**PCA - Summary**

Given $x_i \in \mathbb{R}^n, i = 1,2,\cdots m$ï¼Œ perform PCA by:

- Normalized by the center
- ç”±ä¸­å¿ƒå½’ä¸€åŒ–

$$
\tilde{X} =  [\tilde{x}_1, \cdots , \tilde{x}_m], \tilde{x}_i = x_i - \bar{x}, i = 1,\cdots , m \qquad \bar{x} = \frac{1}{m} \sum_{i=1}^m x_i\cdot 
$$

- Compute SVD $H = \tilde{X}\tilde{X}^T = U_r \Sigma^2 U_r^T$

- The principle vectors are the columns of $ğ‘ˆ_ğ‘Ÿ$
- (Eigenvector of ğ‘‹ = Eigenvector of ğ»)
- ä¸»å‘é‡æ˜¯$ğ‘ˆ_ğ‘Ÿ$çš„åˆ—
- ï¼ˆğ‘‹ çš„ç‰¹å¾å‘é‡ = ğ» çš„ç‰¹å¾å‘é‡ï¼‰

**Dimensionality Reduction é™ç»´**

Given $x_i \in \mathbb{R}^n, i = 1,2, \cdots m$ï¼Œperform PCA to get $l$ principle components $\left \{z_1, z_2, \cdots, z_l \right \}, \ z_j \in \mathbb{R}^n$

ç»™å®šçš„ $x_i \in \mathbb{R}^n, i = 1,2, \cdots m$ æ‰§è¡Œ PCA å¾—åˆ° $ğ‘™$ ä¸»æˆåˆ† $\left \{z_1, z_2, \cdots, z_l \right \}, \ z_j \in \mathbb{R}^n$

- Compress $ğ‘¥_ğ‘–$ from ğ‘› dimension to $ğ‘™$ dimension, with $ğ‘™ \ll ğ‘›$
- å°† $ğ‘¥_ğ‘–$ ä» ğ‘› ç»´åº¦å‹ç¼©åˆ° $ğ‘™$ ç»´åº¦ï¼Œä½¿ç”¨ $ğ‘™ \ll ğ‘›$
- Encoder ç¼–ç å™¨

$$
\begin{bmatrix}a_{i1} \\ . \\. \\. \\ a_{il} \end{bmatrix} = \begin{bmatrix} z_1^T \\ . \\. \\. \\  z_l^T \end{bmatrix} x_i
$$

- Reconstruct $ğ‘¥_ğ‘–$  from the principle components
- ä»ä¸»æˆåˆ†é‡æ„$ğ‘¥_ğ‘–$

- Decoder è§£ç å™¨

$$
\hat{x_i} = \sum_{j=1}^l a_j z_j = [z_1, \cdots, z_l] \begin{bmatrix}a_{i1} \\ . \\. \\. \\ a_{il} \end{bmatrix} 
$$

Point cloud is projected into two principle axis {1, 2}

ç‚¹äº‘æŠ•å½±åˆ°ä¸¤ä¸ªä¸»è½´{1, 2}

![0007.PointCloudProjected](1301pclpics/0007.PointCloudProjected.png)

- Represent a $H \times W$ binary/gray-scale image by a vector $x_i \in \mathbb{R}^n, n = HW$
- ç”¨å‘é‡è¡¨ç¤º $H \times W$ äºŒè¿›åˆ¶/ç°åº¦å›¾åƒ $x_i \in \mathbb{R}^n, n = HW$
- Get the principle vectors $\left \{z_1, \cdots, z_l \right \}, z_j \in \mathbb{R}^n$
- å¾—åˆ°ä¸»å‘é‡ $\left \{z_1, \cdots, z_l \right \}, z_j \in \mathbb{R}^n$
- Digit recognition by clustering over the principle components $a_i = [a_1, \cdots a_l]^T \in \mathbb{R}^l$
- é€šè¿‡åœ¨ä¸»æˆåˆ†ä¸Šèšç±»çš„æ•°å­—è¯†åˆ« $a_i = [a_1, \cdots a_l]^T \in \mathbb{R}^l$
- Similarly, face recognition by Eigenfaces
- åŒæ ·ï¼ŒEigenfaces çš„äººè„¸è¯†åˆ«

![0008.DigitRecognition](1301pclpics/0008.DigitRecognition.png)

# Kernel PCA

- PCA is linear
- How to handle data not linearly separable?
- Lift it to high dimension!
- å°†å…¶æå‡åˆ°é«˜ç»´ï¼

![0009.notLinearlySeparable](1301pclpics/0009.notLinearlySeparable.png)

- Original data

$$
x_i = [x_{i1}, x_{i2}] \in \mathbb{R}^2
$$

- Lifted data

$$
\phi(x_i) = [x_{i1}, x_{i2}, x_{i1}^2 + x_{i2}^2] \in \mathbb{R}^3
$$

- They are separable now.
  - E.g., some principle component of $\phi(ğ‘¥_ğ‘–)$ is able to tell the difference between the red and green
  - ä¾‹å¦‚ï¼Œ$\phi(ğ‘¥_ğ‘–)$ çš„æŸä¸ªä¸»æˆåˆ†èƒ½å¤ŸåŒºåˆ†çº¢è‰²å’Œç»¿è‰²

![0010.LiftData](1301pclpics/0010.LiftData.png)

- Input data $ğ‘¥_ğ‘– \in \mathbb{R}^{n_0}$ , non-linear mapping $\phi: \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_1}$

- Follow the standard Linear PCA on the lifted space $\mathbb{R}^{n_1}$

- åœ¨æå‡ç©ºé—´ä¸Šéµå¾ªæ ‡å‡†çº¿æ€§ PCA $\mathbb{R}^{n_1}$

  - 1.Assume $\phi(x_i)$is already zero-center

  - $$
    \frac{1}{N} \sum_{i=1}^N \phi(x_i) = 0
    $$

  - 2.Compute correlation matrix è®¡ç®—ç›¸å…³çŸ©é˜µ

  - $$
    \tilde{H} = \frac{1}{N} \sum_{i=1}^N \phi (x_i) \phi^T(x_i)
    $$

  - 3.Solve the eigenvectors/eigenvalues by  æ±‚è§£ç‰¹å¾å‘é‡/ç‰¹å¾å€¼

  - $$
    \tilde{H} \tilde{z} = \tilde{\lambda} \tilde{z}
    $$

- Problem solved? No fully.

  - How to define $\phi$?
  - Can we avoid working with the high dimension data? å¯ä»¥é¿å…ä½¿ç”¨é«˜ç»´æ•°æ®å—



- Note that eigenvectors can be expressed as linear combination of features

- è¯·æ³¨æ„ï¼Œç‰¹å¾å‘é‡å¯ä»¥è¡¨ç¤ºä¸ºç‰¹å¾çš„çº¿æ€§ç»„åˆ

- $$
  \tilde{z} = \sum_{j=1}^N \alpha_j \phi(x_j)
  $$

- proof

- $$
  \tilde{H} \tilde{z} = \tilde{\lambda} \tilde{z}   \\
  \frac{1}{N} \sum_{i=1}^N \phi(x_i) \phi^T(x_i) \tilde{z} = \tilde{\lambda} \tilde{z}  \\
  scalar:\phi^T(x_i) \tilde{z}
  $$

- Find the eigenvector $\tilde{ğ‘§} =$ find the coefficient $\alpha_ğ‘—$ 



- æ±‚ç‰¹å¾å‘é‡ $\tilde{ğ‘§} =$ æ±‚ç³»æ•° $\alpha_ğ‘—$

- Put that linear combination into $\tilde{H} \tilde{z} = \tilde{\lambda} \tilde{z}$ 

- $$
  \frac{1}{N} \sum_{i=1}^N \phi(x_i) \phi^T(x_i) (\sum_{j=1}^N \alpha_j \phi(x_j)) = \tilde{\lambda}\sum_{j=1}^N \alpha_j \phi(x_j) \\
  \frac{1}{N} \sum_{i=1}^N \phi(x_i) (\sum_{j=1}^N \alpha_j \phi^T(x_i) \phi(x_j)) = \tilde{\lambda}\sum_{j=1}^N \alpha_j \phi(x_j)
  $$

- Letâ€™s define kernel function $k(x_i, x_j) = \phi^T(x_i) \phi(x_j)$

- $$
  \frac{1}{N} \sum_{i=1}^N \phi(x_i) (\sum_{j=1}^N \alpha_j k(x_i, x_j)) = \tilde{\lambda} \sum_{j=1}^N \alpha_j \phi(x_j)
  $$

- Multiply both sides by ä¸¤è¾¹ä¹˜ä»¥ $\phi(x_k), k = 1, \cdots, N$  

- $$
  \sum_{i=1}^N \sum_{j=1}^N \alpha_j k(x_k, x_i) k(x_i, x_j) = N \tilde{\lambda} \sum_{j=1}^N \alpha_j k(x_k, x_j), k = 1, \cdots, N
  $$





- Now define the Gram matrix $K \in \mathbb{R}^{N \times N}, K(i, j) = k(x_i, x_j)$    

  - $ğ¾$ is symmetric because $k(x_i, x_j) = k(x_j, x_i)$

- The above equation can be written as

- $$
  K^2 \alpha = N \tilde{\lambda} K \alpha
  $$

- Remove ğ¾ on both sides

- $$
  K \alpha = N \tilde{\lambda} \alpha \\
  K \alpha = \lambda \alpha
  $$

- Again, get the eigenvectors $\alpha_ğ‘Ÿ$ and eigenvalues $\lambda_r, r=1, \cdots, l$

- å¢ç›Šï¼Œå¾—åˆ°ç‰¹å¾å‘é‡ $\alpha_ğ‘Ÿ$ å’Œç‰¹å¾å€¼ $\lambda_r, r=1, \cdots, l$

- However, we have to ensure that $\tilde{ğ‘§}$ is unit vector, Note that we are solving the linear PCA in the feature space

- ä½†æ˜¯ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿ $\tilde{ğ‘§}$ æ˜¯å•ä½å‘é‡ï¼Œæ³¨æ„æˆ‘ä»¬æ˜¯åœ¨ç‰¹å¾ç©ºé—´ä¸­æ±‚è§£çº¿æ€§ PCA

- $$
  \tilde{H} \tilde{z} = \tilde{\lambda} \tilde{z} \qquad \tilde{z} = \sum_{j=1}^N \alpha_j \phi(x_j)
  $$





- The normalization of $\tilde{ğ‘§}$ leads to

- $$
  1 = \tilde{z}_r^T \tilde{z}_r \\
  1 = \sum_{i=1}^N \sum_{j=1}^N \alpha_{ri} \alpha_{rj} \phi^T(x_i) \phi(x_j) \\
  1 = \alpha_r^T K \alpha_r
  $$

- Note that $K \alpha = \lambda \alpha$,  we have $\alpha_r^T \lambda_r \alpha_r = 1, \forall r$  

- That is, normalize $\alpha_ğ‘Ÿ$ to be norm $1/\lambda_ğ‘Ÿ$ 

- Now, the $ğ‘Ÿ^{ğ‘¡â„}$ principle vector in the lifted space is given below, which is unknown

- ç°åœ¨ï¼Œæå‡ç©ºé—´ä¸­çš„$ğ‘Ÿ^{ğ‘¡â„}$ä¸»å‘é‡å¦‚ä¸‹

- $$
  \tilde{z}_r = \sum_{j=1}^N \alpha_{rj} \phi(x_j)
  $$





- Now, the $ğ‘Ÿ^{ğ‘¡â„}$ principle vector in the lifted space is given below

- ç°åœ¨ï¼Œæå‡ç©ºé—´ä¸­çš„$ğ‘Ÿ^{ğ‘¡â„}$ä¸»å‘é‡å¦‚ä¸‹

- $$
  \tilde{z}_r = \sum_{j=1}^N \alpha_{rj} \phi(x_j)
  $$

  

- But we know the projection of data point $ğ‘¥$ projected into principle component $z_r$

- ä½†æ˜¯æˆ‘ä»¬çŸ¥é“æŠ•å½±åˆ°ä¸»åˆ†é‡ $z_r$ çš„æ•°æ®ç‚¹ $ğ‘¥$ çš„æŠ•å½±

- $$
  \phi^T(x) \tilde{z}_r = \sum_{j=1}^N \alpha_{rj} \phi^T(x) \phi(x_j) = \sum_{j=1}^N \alpha_{rj} k(x, x_j)
  $$

- One more thing, we assume $\phi(ğ‘¥_ğ‘–)$ is of zero mean.

- è¿˜æœ‰ä¸€ä»¶äº‹ï¼Œæˆ‘ä»¬å‡è®¾  $\phi(ğ‘¥_ğ‘–)$ æ˜¯é›¶å‡å€¼ã€‚





- Normalize $\phi(ğ‘¥_ğ‘–)$  to be zero mean

- å°† $\phi(ğ‘¥_ğ‘–)$ å½’ä¸€åŒ–ä¸ºé›¶å‡å€¼

- $$
  \tilde{\phi} (x_i) = \phi (x_i) - \frac{1}{N} \sum_{j=1}^N \phi(x_j)
  $$

- The normalized kernel $\tilde{k}(x_i, x_j)$ is given by

- å½’ä¸€åŒ–å†…æ ¸ $\tilde{k}(x_i, x_j)$ ç”±ä¸‹å¼ç»™å‡º

- $$
  \tilde{k}(x_i, x_j) = \tilde{\phi}^T (x_i) \tilde{\phi} (x_j) \\
  = (\phi(x_i) - \frac{1}{N} \sum_{k=1}^N \phi(x_k))^T (\phi(x_j) - \frac{1}{N} \sum_{l=1}^N \phi(x_l)) \\
  = k(x_i, x_j) - \frac{1}{N} \sum_{k=1}^N k(x_i, x_k) - \frac{1}{N} \sum_{k=1}^N k(x_j, x_k) + \frac{1}{N^2} \sum_{k=1}^N \sum_{l=1}^N k(x_k, x_l)
  $$

- In the matrix form åœ¨çŸ©é˜µå½¢å¼ $\tilde{K} = K - 2 \mathbb{I}_{\frac{1}{N}} + \mathbb{I}_{\frac{1}{N}} K \mathbb{I}_{\frac{1}{N}}, where \ \mathbb{I}_{\frac{1}{N}}(i, j) = \frac{1}{N}, \forall i,j$



- Kernel choices
  - Linear $k(x_i, x_j) = x_i^T x_j$ 
  - Polynomial $k(x_i, x_j) = (1 + x_i^Tx_j)^p$ 
  - Gaussian $k(x_i, x_j) = e^{- \beta \|x_i - x_j \|_2}$  
  - Laplacian $k(x_i, x_j) = e^{- \beta \|x_i - x_j \|_1}$   
- Usually choose by experiments if there is no explicit knowledge what kernels best separate the data points.
- å¦‚æœæ²¡æœ‰æ˜ç¡®çš„çŸ¥è¯†ï¼Œé€šå¸¸é€šè¿‡å®éªŒé€‰æ‹©ä»€ä¹ˆå†…æ ¸æœ€å¥½åœ°åˆ†ç¦»æ•°æ®ç‚¹ã€‚



- Select a kernel $ğ‘˜(ğ‘¥_ğ‘–, ğ‘¥_ğ‘—)$ , compute the Gram matrix $ğ¾(ğ‘–, ğ‘—) = ğ‘˜(ğ‘¥_ğ‘–, ğ‘¥_ğ‘— )$  

- é€‰æ‹©ä¸€ä¸ªæ ¸ $ğ‘˜(ğ‘¥_ğ‘–, ğ‘¥_ğ‘—)$ ï¼Œè®¡ç®— Gram çŸ©é˜µ $ğ¾(ğ‘–, ğ‘—) = ğ‘˜(ğ‘¥_ğ‘–, ğ‘¥_ğ‘— )$   

- Normalize ğ¾

- $$
  \tilde{K} = K - 2 \mathbb{I}_{\frac{1}{N}} K + \mathbb{I}_{\frac{1}{N}} K \mathbb{I}_{\frac{1}{N}}
  $$

- Solve the eigenvector/eigenvalues of $\tilde{K}$ 

- æ±‚è§£ $\tilde{K}$ çš„ç‰¹å¾å‘é‡/ç‰¹å¾å€¼  

- $$
  \tilde{K} \alpha_r = \lambda_r \alpha_r
  $$

- Normalize $\alpha_r$ to be $\alpha_r^T \alpha_r = \frac{1}{\lambda_r}$ 

- For any data point $x \in \mathbb{R}^n$ , compute its projection onto $r^{th}$ principle component $y_r \in \mathbb{R}$  

- å¯¹äºä»»ä½•æ•°æ®ç‚¹ $x \in \mathbb{R}^n$ ï¼Œè®¡ç®—å…¶æŠ•å½±åˆ° $r^{th}$ ä¸»åˆ†é‡ $y_r \in \mathbb{R}$

- $$
  y_r = \phi^T(x)\tilde{z}_r = \sum_{j=1}^N \alpha_{rj} k(x, x_j)
  $$

  

Input data is not separable by linear PCA

![0011.NoLinearPCA](1301pclpics/0011.NoLinearPCA.png)



- Projection into $1^{st}$ and $2^{nd}$ principle components
- æŠ•å½±åˆ° $1^{st}$ å’Œ $2^{nd}$ ä¸»æˆåˆ†
- k PCA polynomial kernel $k(x_i, x_j) = (1 + x_i^T x_j)^2$
- Points can be separated by the first projection $ğ‘¦_0$
- ç‚¹å¯ä»¥é€šè¿‡ç¬¬ä¸€ä¸ªæŠ•å½±$ğ‘¦_0$åˆ†å¼€

![0012.kPCAPolynomialKernel](1301pclpics/0012.kPCAPolynomialKernel.png)



- Projection into $1^{st}$ and $2^{nd}$ principle components
- k PCA Gaussian kernel $k(x_i, x_j) = e^{- \beta \|x_i - x_j \|_2}$
- Points can be separated by the first projection $ğ‘¦_0$

![0013.kPCAGaussianKernel](1301pclpics/0013.kPCAGaussianKernel.png)

